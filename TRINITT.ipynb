{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codx-aks/TriNitt-ML/blob/main/TRINITT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5sX-TxMdgaq"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from textwrap import wrap\n",
        "import pandas as pd\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import (\n",
        "    GRU,\n",
        "    Add,\n",
        "    AdditiveAttention,\n",
        "    Attention,\n",
        "    Concatenate,\n",
        "    Dense,\n",
        "    Embedding,\n",
        "    LayerNormalization,\n",
        "    Reshape,\n",
        "    StringLookup,\n",
        "    TextVectorization,\n",
        ")\n",
        "\n",
        "print(tf.version.VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNZ1kFFtxW_h"
      },
      "outputs": [],
      "source": [
        "# Change these to control the accuracy/speed\n",
        "VOCAB_SIZE = 10000  # use fewer words to speed up convergence\n",
        "ATTENTION_DIM = 512  # size of dense layer in Attention\n",
        "WORD_EMBEDDING_DIM = 128\n",
        "\n",
        "# InceptionResNetV2 takes (224, 224, 3) image as inputs\n",
        "# and return features in (5, 5, 1536) shape\n",
        "FEATURE_EXTRACTOR = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n",
        "    include_top=False, weights=\"imagenet\"\n",
        ")\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "IMG_CHANNELS = 3\n",
        "FEATURES_SHAPE = (5, 5, 1536)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCvcvhc8EzN",
        "outputId": "b71d2b01-f494-42a3-c720-9bcef79e65d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee-dkPrDKpmV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import ast\n",
        "import io\n",
        "\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "def decode_and_resize_image(image_bytes):\n",
        "    img = tf.image.decode_jpeg(image_bytes, channels=3)\n",
        "    img = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
        "    # Normalize pixel values\n",
        "    img = img / 255.0\n",
        "    return img\n",
        "\n",
        "def get_image_label(filename, caption, img):\n",
        "    caption_list = [caption] if not isinstance(caption, list) else caption\n",
        "    first_caption = caption_list[0]\n",
        "    print(len(caption_list))\n",
        "    return {\"image_tensor\": img, \"caption\": first_caption}\n",
        "\n",
        "\n",
        "# Define mapping function\n",
        "def map_fn(filename, caption, image_bytes):\n",
        "    # Decode and resize image\n",
        "    print(\"Type of image_bytes:\", type(image_bytes))\n",
        "    print(\"insideeee\")\n",
        "    img = decode_and_resize_image(image_bytes)\n",
        "    return get_image_label(filename, caption, img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataset\n",
        "dataset_path = \"/content/drive/My Drive/archive/train.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Parse the stringified dictionaries in the \"image\" column\n",
        "df['image'] = df['image'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to extract the image bytes from the nested dictionary\n",
        "def extract_image_bytes(image):\n",
        "    return image.get(\"bytes\", None)\n",
        "\n",
        "# Extract image bytes from the \"image\" column\n",
        "df[\"image_bytes\"] = df[\"image\"].apply(extract_image_bytes)\n",
        "\n",
        "print(\"Data type of image_bytes:\", df[\"image_bytes\"].dtype)\n",
        "print(\"Shape of image_bytes:\", df[\"image_bytes\"].shape)\n",
        "print(\"Head of image_bytes:\")\n",
        "print(df[\"image_bytes\"].head())\n",
        "\n",
        "# Create dataset\n",
        "trainds = tf.data.Dataset.from_tensor_slices((df[\"filename\"], df[\"captions\"], df[\"image_bytes\"]))\n",
        "# Apply mapping function to the dataset\n",
        "trainds = trainds.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE).shuffle(BUFFER_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "bHb4_PmugTdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read dataset\n",
        "dataset_path_test = \"/content/drive/My Drive/archive/test.csv\"\n",
        "dftest = pd.read_csv(dataset_path_test)\n",
        "\n",
        "# Parse the stringified dictionaries in the \"image\" column\n",
        "dftest['image'] = dftest['image'].apply(ast.literal_eval)\n",
        "\n",
        "# Extract image bytes from the \"image\" column\n",
        "dftest[\"image_bytes\"] = dftest[\"image\"].apply(extract_image_bytes)\n",
        "\n",
        "print(\"Data type of image_bytes:\", dftest[\"image_bytes\"].dtype)\n",
        "print(\"Shape of image_bytes:\", dftest[\"image_bytes\"].shape)\n",
        "print(\"Head of image_bytes:\")\n",
        "print(dftest[\"image_bytes\"].head())\n",
        "\n",
        "# Create dataset\n",
        "testds = tf.data.Dataset.from_tensor_slices((dftest[\"filename\"], dftest[\"captions\"], dftest[\"image_bytes\"]))\n",
        "\n",
        "# Apply mapping function to the dataset\n",
        "testds = testds.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE).shuffle(BUFFER_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "l0jP3f7jllP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read dataset\n",
        "dataset_path_val = \"/content/drive/My Drive/archive/valid.csv\"\n",
        "dfval = pd.read_csv(dataset_path_val)\n",
        "\n",
        "# Parse the stringified dictionaries in the \"image\" column\n",
        "dfval['image'] = dfval['image'].apply(ast.literal_eval)\n",
        "\n",
        "# Extract image bytes from the \"image\" column\n",
        "dfval[\"image_bytes\"] = dfval[\"image\"].apply(extract_image_bytes)\n",
        "\n",
        "print(\"Data type of image_bytes:\", dfval[\"image_bytes\"].dtype)\n",
        "print(\"Shape of image_bytes:\", dfval[\"image_bytes\"].shape)\n",
        "print(\"Head of image_bytes:\")\n",
        "print(dfval[\"image_bytes\"].head())\n",
        "\n",
        "# Create dataset\n",
        "valds = tf.data.Dataset.from_tensor_slices((dfval[\"filename\"], dfval[\"captions\"], dfval[\"image_bytes\"]))\n",
        "\n",
        "# Apply mapping function to the dataset\n",
        "valds = valds.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE).shuffle(BUFFER_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "IzJhfDymmBUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jHC1hQpKvbS"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fetch data example\n",
        "for data in valds.take(4):\n",
        "    byte_array = data['image_tensor']\n",
        "    # Convert to NumPy array using TensorFlow's NumPy compatibility feature\n",
        "    byte_array_np = tf.experimental.numpy.asarray(byte_array)\n",
        "    # Convert back to regular NumPy array for image processing\n",
        "    byte_array_np = np.array(byte_array_np)\n",
        "    image = (byte_array_np * 255).astype(np.uint8)\n",
        "\n",
        "    # Extract the caption\n",
        "    caption = data['caption'].numpy().decode('utf-8')  # Decode bytes to string\n",
        "\n",
        "    # Display the image and caption\n",
        "    plt.imshow(image)\n",
        "    plt.title(caption)\n",
        "    plt.axis('off')  # Turn off axis\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xY0128FKzeT"
      },
      "outputs": [],
      "source": [
        "def add_start_end_token(data):\n",
        "    start = tf.convert_to_tensor(\"<start>\")\n",
        "    end = tf.convert_to_tensor(\"<end>\")\n",
        "    data[\"caption\"] = tf.strings.join(\n",
        "        [start, data[\"caption\"], end], separator=\" \"\n",
        "    )\n",
        "    return data\n",
        "\n",
        "\n",
        "trainds = trainds.map(add_start_end_token)\n",
        "testds = testds.map(add_start_end_token)\n",
        "valds = valds.map(add_start_end_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOrt0x_kLJoV"
      },
      "outputs": [],
      "source": [
        "MAX_CAPTION_LEN = 128\n",
        "\n",
        "\n",
        "# We will override the default standardization of TextVectorization to preserve\n",
        "# \"<>\" characters, so we preserve the tokens for the <start> and <end>.\n",
        "def standardize(inputs):\n",
        "    inputs = tf.strings.lower(inputs)\n",
        "    return tf.strings.regex_replace(\n",
        "        inputs, r\"[!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~]?\", \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Choose the most frequent words from the vocabulary & remove punctuation etc.\n",
        "tokenizer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    standardize=standardize,\n",
        "    output_sequence_length=MAX_CAPTION_LEN,\n",
        ")\n",
        "\n",
        "tokenizer.adapt(trainds.map(lambda x: x[\"caption\"]))\n",
        "tokenizer.adapt(valds.map(lambda x: x[\"caption\"]))\n",
        "tokenizer.adapt(testds.map(lambda x: x[\"caption\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4w_6JbALMd3"
      },
      "outputs": [],
      "source": [
        "tokenizer([\"<start> This is a sentence <end>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vygKqIPBLO2-"
      },
      "outputs": [],
      "source": [
        "sample_captions = []\n",
        "for d in trainds.take(5):\n",
        "    sample_captions.append(d[\"caption\"].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZt4vi_7LXKt"
      },
      "outputs": [],
      "source": [
        "for wordid in tokenizer([sample_captions[0]])[0]:\n",
        "    print(tokenizer.get_vocabulary()[wordid], end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHSHjoouLX6B"
      },
      "outputs": [],
      "source": [
        "# Lookup table: Word -> Index\n",
        "word_to_index = StringLookup(\n",
        "    mask_token=\"\", vocabulary=tokenizer.get_vocabulary()\n",
        ")\n",
        "\n",
        "# Lookup table: Index -> Word\n",
        "index_to_word = StringLookup(\n",
        "    mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY5DtJQ9LbEw"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "\n",
        "def create_ds_fn(data):\n",
        "    img_tensor = data[\"image_tensor\"]\n",
        "    caption = tokenizer(data[\"caption\"])\n",
        "\n",
        "    target = tf.roll(caption, -1, 0)\n",
        "    zeros = tf.zeros([1], dtype=tf.int64)\n",
        "    target = tf.concat((target[:-1], zeros), axis=-1)\n",
        "    return (img_tensor, caption), target\n",
        "\n",
        "\n",
        "batched_ds = (\n",
        "    trainds.map(create_ds_fn)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "batched_val_ds = (\n",
        "    valds.map(create_ds_fn)\n",
        "    .batch(128, drop_remainder=True)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "batched_test_ds = (\n",
        "    testds.map(create_ds_fn)\n",
        "    .batch(128, drop_remainder=True)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7olM63LLiB9"
      },
      "outputs": [],
      "source": [
        "print(len(batched_val_ds))\n",
        "for (img, caption), label in batched_val_ds.take(2):\n",
        "    print(f\"Image shape: {img.shape}\")\n",
        "    print(f\"Caption shape: {caption.shape}\")\n",
        "    print(f\"Label shape: {label.shape}\")\n",
        "    print(caption[0])\n",
        "    print(label[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxdpEyEQLlD3"
      },
      "outputs": [],
      "source": [
        "FEATURE_EXTRACTOR.trainable = False\n",
        "\n",
        "image_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "image_features = FEATURE_EXTRACTOR(image_input)\n",
        "\n",
        "x = Reshape((FEATURES_SHAPE[0] * FEATURES_SHAPE[1], FEATURES_SHAPE[2]))(image_features)\n",
        "encoder_output = Dense(ATTENTION_DIM, activation=\"relu\")(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZeLl8ebLoUa"
      },
      "outputs": [],
      "source": [
        "encoder = tf.keras.Model(inputs=image_input, outputs=encoder_output)\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t8PIRR5LviD"
      },
      "outputs": [],
      "source": [
        "word_input = Input(shape=(MAX_CAPTION_LEN), name=\"words\")\n",
        "embed_x = Embedding(VOCAB_SIZE, ATTENTION_DIM)(word_input)\n",
        "\n",
        "decoder_gru = GRU(\n",
        "    ATTENTION_DIM,\n",
        "    return_sequences=True,\n",
        "    return_state=True,\n",
        ")\n",
        "gru_output, gru_state = decoder_gru(embed_x)\n",
        "\n",
        "decoder_attention = Attention()\n",
        "context_vector = decoder_attention([gru_output, encoder_output])\n",
        "\n",
        "addition = Add()([gru_output, context_vector])\n",
        "\n",
        "layer_norm = LayerNormalization(axis=-1)\n",
        "layer_norm_out = layer_norm(addition)\n",
        "\n",
        "decoder_output_dense = Dense(VOCAB_SIZE)\n",
        "decoder_output = decoder_output_dense(layer_norm_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSvh1iJXLwYo"
      },
      "outputs": [],
      "source": [
        "decoder = tf.keras.Model(\n",
        "    inputs=[word_input, encoder_output], outputs=decoder_output\n",
        ")\n",
        "# tf.keras.utils.plot_model(decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVUPN2NmL1la"
      },
      "outputs": [],
      "source": [
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6ohhtXIL4lG"
      },
      "outputs": [],
      "source": [
        "image_caption_train_model = tf.keras.Model(\n",
        "    inputs=[image_input, word_input], outputs=decoder_output\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0TeeMwyL7Uz"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    # returns 1 to word index and 0 to padding (e.g. [1,1,1,1,1,0,0,0,0,...,0])\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int32)\n",
        "    sentence_len = tf.reduce_sum(mask)\n",
        "    loss_ = loss_[:sentence_len]\n",
        "\n",
        "    return tf.reduce_mean(loss_, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsChe_k_L-Jc"
      },
      "outputs": [],
      "source": [
        "image_caption_train_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=loss_function,\n",
        "     metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "kviA8O74YjaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate sacrebleu"
      ],
      "metadata": {
        "id": "vtxB7dKQQ5PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMMh003HMBFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d120b56a-8702-47cc-f4a5-77a081221dfa"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29/34 [========================>.....] - ETA: 8:16 - loss: 3.3487 - accuracy: 0.5428"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "# history = image_caption_train_model.fit(batched_ds, epochs=1)\n",
        "EPOCHS=1\n",
        "# Train the model\n",
        "history = image_caption_train_model.fit(batched_ds, epochs=EPOCHS, validation_data=batched_val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcqmfJJiBzAk"
      },
      "outputs": [],
      "source": [
        "image_caption_train_model.save('/content/drive/My Drive/gru_model2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = image_caption_train_model.evaluate(batched_test_ds)\n",
        "\n",
        "print(\"Test loss:\", test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQcZzVcfv6a0",
        "outputId": "9d83b872-0b49-4093-f08a-e7acd10b8c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/8 [==============>...............] - ETA: 2:36 - loss: 2.9331 - accuracy: 0.5218"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# Plot the training and validation accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accuracy, label='Training Accuracy', color='blue')\n",
        "plt.plot(val_accuracy, label='Validation Accuracy', color='red')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss, label='Training Loss', color='blue')\n",
        "plt.plot(val_loss, label='Validation Loss', color='red')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-GCvl4euBdrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "\n",
        "def create_ds_fn(data):\n",
        "    img_tensor = data[\"image_tensor\"]\n",
        "    caption = tokenizer(data[\"caption\"])\n",
        "\n",
        "    target = tf.roll(caption, -1, 0)\n",
        "    zeros = tf.zeros([1], dtype=tf.int64)\n",
        "    target = tf.concat((target[:-1], zeros), axis=-1)\n",
        "    return (img_tensor, caption), target\n",
        "\n",
        "\n",
        "batched_val_ds = (\n",
        "    valds.map(create_ds_fn)\n",
        "    .batch(128, drop_remainder=True)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "batched_test_ds = (\n",
        "    testds.map(create_ds_fn)\n",
        "    .batch(128, drop_remainder=True)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "xjmb3K_tiM2o",
        "outputId": "73ae0802-9c33-4c96-8d8c-21c31be1ec06"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "in user code:\n\n    File \"<ipython-input-14-6eb0561b4ecf>\", line 6, in create_ds_fn  *\n        caption = tokenizer(data[\"caption\"])\n\n    NameError: name 'tokenizer' is not defined\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6eb0561b4ecf>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m batched_val_ds = (\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mvalds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_ds_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2278\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2279\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2280\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2281\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     35\u001b[0m       warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m     36\u001b[0m                     \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m---> 37\u001b[0;31m     return _MapDataset(\n\u001b[0m\u001b[1;32m     38\u001b[0m         input_dataset, map_func, preserve_cardinality=True, name=name)\n\u001b[1;32m     39\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# Note: wrapper_helper will apply autograph based on context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filerg5wckk7.py\u001b[0m in \u001b[0;36mtf__create_ds_fn\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_tensor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caption'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mzeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: in user code:\n\n    File \"<ipython-input-14-6eb0561b4ecf>\", line 6, in create_ds_fn  *\n        caption = tokenizer(data[\"caption\"])\n\n    NameError: name 'tokenizer' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    # returns 1 to word index and 0 to padding (e.g. [1,1,1,1,1,0,0,0,0,...,0])\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int32)\n",
        "    sentence_len = tf.reduce_sum(mask)\n",
        "    loss_ = loss_[:sentence_len]\n",
        "\n",
        "    return tf.reduce_mean(loss_, 1)\n",
        "\n",
        "saved_model_path = '/content/drive/My Drive/gru_model.h5'\n",
        "loaded_model = tf.keras.models.load_model(saved_model_path,custom_objects={'loss_function': loss_function})\n",
        "\n",
        "# Assuming batched_val_ds is your validation dataset\n",
        "# Make predictions on the validation dataset\n",
        "predictions_val = loaded_model.predict(valds)\n",
        "\n",
        "# Convert predictions to a DataFrame\n",
        "predictions_dfval = pd.DataFrame(predictions_val)\n",
        "\n",
        "# Save DataFrame to a CSV file\n",
        "csv_filename_val = \"/content/drive/My Drive/validation_predictions.csv\"\n",
        "predictions_dfval.to_csv(csv_filename_val, index=False)\n",
        "\n",
        "print(\"Shape of validation predictions:\", predictions_val.shape)\n",
        "print(\"Predictions saved to:\", csv_filename_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "FthuSgl_wh7C",
        "outputId": "50ee2245-e53c-41b8-91b0-5a92e726c074"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 197, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Missing data for input \"input_7\". You passed a data dictionary with keys ['image_tensor', 'caption']. Expected the following keys: ['input_7', 'words']\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-de5eb880bf31>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Assuming batched_val_ds is your validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Make predictions on the validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mpredictions_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Convert predictions to a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 197, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Missing data for input \"input_7\". You passed a data dictionary with keys ['image_tensor', 'caption']. Expected the following keys: ['input_7', 'words']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_test = image_caption_train_model.predict(batched_test_ds)\n",
        "\n",
        "predictions_dftest = pd.DataFrame(predictions_test)\n",
        "\n",
        "# Save DataFrame to a CSV file\n",
        "csv_filename_test = \"/content/drive/My Drive/test_predictions.csv\"\n",
        "predictions_dftest.to_csv(csv_filename_test, index=False)\n",
        "print(\"Shape of test predictions:\", predictions_test.shape)"
      ],
      "metadata": {
        "id": "SvLMTrQMwkxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def tokenizer_config_to_json(tokenizer_config):\n",
        "    serialized_config = {}\n",
        "    for key, value in tokenizer_config.items():\n",
        "        if not callable(value):  # Exclude functions from serialization\n",
        "            if isinstance(value, dict):\n",
        "                serialized_value = tokenizer_config_to_json(value)  # Recursive call for nested dictionaries\n",
        "            else:\n",
        "                serialized_value = value\n",
        "            serialized_config[key] = serialized_value\n",
        "    return serialized_config\n",
        "\n",
        "file_path = \"/content/drive/My Drive/tokenizer.json\"\n",
        "tokenizer_config_json = tokenizer_config_to_json(tokenizer.get_config())\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(tokenizer_config_json, json_file)"
      ],
      "metadata": {
        "id": "EqIMmMwwCxev"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "with open(file_path, 'r') as json_file:\n",
        "    loaded_tokenizer_config = json.load(json_file)\n",
        "print(loaded_tokenizer_config)\n",
        "def custom_standardization(inputs):\n",
        "    inputs = tf.strings.lower(inputs)\n",
        "    return tf.strings.regex_replace(\n",
        "        inputs, r\"[!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~]?\", \"\"\n",
        "    )\n",
        "# Reconstruct tokenizer from configuration\n",
        "tokenizer = TextVectorization.from_config(loaded_tokenizer_config)\n",
        "tokenizer.standardize = custom_standardization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "AZBXTHOwa0iH",
        "outputId": "82d18082-6423-43ff-b17b-a3e8ae3fae16"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'text_vectorization_1', 'trainable': True, 'dtype': 'string', 'batch_input_shape': [], 'max_tokens': 20000, 'split': 'whitespace', 'ngrams': None, 'output_mode': 'int', 'output_sequence_length': 128, 'pad_to_max_tokens': False, 'sparse': False, 'ragged': False, 'vocabulary': None, 'idf_weights': None, 'encoding': 'utf-8', 'vocabulary_size': 2185}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'standardize'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-e6ee63249fa0>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Reconstruct tokenizer from configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextVectorization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_tokenizer_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_standardization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/preprocessing/text_vectorization.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         if config[\"standardize\"] not in (\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0mLOWER_AND_STRIP_PUNCTUATION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mLOWER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'standardize'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecqckvjnMDug"
      },
      "outputs": [],
      "source": [
        "gru_state_input = Input(shape=(ATTENTION_DIM), name=\"gru_state_input\")\n",
        "\n",
        "# Reuse trained GRU, but update it so that it can receive states.\n",
        "gru_output, gru_state = decoder_gru(embed_x, initial_state=gru_state_input)\n",
        "\n",
        "# Reuse other layers as well\n",
        "context_vector = decoder_attention([gru_output, encoder_output])\n",
        "addition_output = Add()([gru_output, context_vector])\n",
        "layer_norm_output = layer_norm(addition_output)\n",
        "\n",
        "decoder_output = decoder_output_dense(layer_norm_output)\n",
        "\n",
        "# Define prediction Model with state input and output\n",
        "decoder_pred_model = tf.keras.Model(\n",
        "    inputs=[word_input, gru_state_input, encoder_output],\n",
        "    outputs=[decoder_output, gru_state],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image\n",
        "import json\n",
        "def decode_predictions(predictions):\n",
        "    generated_captions = []\n",
        "    with open('tokenizer.json', 'r') as json_file:\n",
        "        tokenizer_json = json.load(json_file)\n",
        "    tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)\n",
        "    for prediction in predictions:\n",
        "        caption_words = []\n",
        "        for token_id in prediction:\n",
        "            word = tokenizer.index_word.get(token_id, '<UNK>')\n",
        "            caption_words.append(word)\n",
        "            if word == '<end>':\n",
        "                break\n",
        "        caption = ' '.join(caption_words[:-1])\n",
        "        generated_captions.append(caption)\n",
        "\n",
        "    return generated_captions\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERMH7-24uoem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu>=1.4.12"
      ],
      "metadata": {
        "id": "dcvHPyJEPzrK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4YRYG-LShiN",
        "outputId": "d72750d4-6621-4de6-d611-d7f6f31c50b7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def compute_bleu_scores(predictions, references):\n",
        "    references = [[ref.split()] for ref in references]\n",
        "    predictions = [pred.split() for pred in predictions]\n",
        "    bleu_score = corpus_bleu(references, predictions)\n",
        "    return bleu_score\n",
        "\n",
        "def evaluate_model(model, dataset):\n",
        "    total_bleu_score = 0\n",
        "    references_list = []\n",
        "    predictions_list = []\n",
        "\n",
        "    # Batch the dataset before iterating over it\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    for batch in dataset:\n",
        "        image_tensors, captions = batch['image_tensor'], batch['caption']\n",
        "        for image_tensor, caption in zip(image_tensors, captions):\n",
        "            # Predict caption for the current image\n",
        "            predicted_caption = predict_caption(tf.expand_dims(image_tensor, axis=0))\n",
        "\n",
        "            # Add reference and prediction for BLEU score calculation\n",
        "            references_list.append([caption.numpy().decode(\"utf-8\")])\n",
        "            predictions_list.append(predicted_caption)\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu_score = compute_bleu_scores(predictions_list, references_list)\n",
        "\n",
        "    return bleu_score\n"
      ],
      "metadata": {
        "id": "pbbFtYeg1YHn"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "thEbmuWzTcOI",
        "outputId": "065c4ffa-98a4-46ad-eb3a-8e8606921015"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-62f13835-cccc-479b-813c-3e4422b5099b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-62f13835-cccc-479b-813c-3e4422b5099b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving WhatsApp Image 2024-03-10 at 02.06.12.jpeg to WhatsApp Image 2024-03-10 at 02.06.12.jpeg\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIzyx6PhMK-T"
      },
      "outputs": [],
      "source": [
        "MINIMUM_SENTENCE_LENGTH = 5\n",
        "\n",
        "\n",
        "## Probabilistic prediction using the trained model\n",
        "def predict_caption(filename):\n",
        "    gru_state = tf.zeros((1, ATTENTION_DIM))\n",
        "\n",
        "    img = tf.image.decode_jpeg(tf.io.read_file(filename), channels=IMG_CHANNELS)\n",
        "    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
        "    img = img / 255\n",
        "\n",
        "    features = encoder(tf.expand_dims(img, axis=0))\n",
        "    dec_input = tf.expand_dims([word_to_index(\"<start>\")], 1)\n",
        "    result = []\n",
        "    for i in range(MAX_CAPTION_LEN):\n",
        "        predictions, gru_state = decoder_pred_model([dec_input, gru_state, features])\n",
        "\n",
        "        # Get the index with maximum probability\n",
        "        predicted_id = tf.argmax(predictions[0][0]).numpy()\n",
        "\n",
        "        result.append(tokenizer.get_vocabulary()[predicted_id])\n",
        "\n",
        "        if predicted_id == word_to_index(\"<end>\"):\n",
        "            break\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 1)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_caption_train_model.summary()\n"
      ],
      "metadata": {
        "id": "Zif0CC175aOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRhmnCTmTddj"
      },
      "outputs": [],
      "source": [
        "filename = \"/content/WhatsApp Image 2024-03-10 at 02.06.12.jpeg\"\n",
        "for i in range(5):\n",
        "    image, caption = predict_caption(filename)\n",
        "    print(\" \".join(caption[:-1]) + \".\")\n",
        "\n",
        "img = tf.image.decode_jpeg(tf.io.read_file(filename), channels=IMG_CHANNELS)\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "saved_model_path = \"/content/drive/My Drive/gru_model.h5\"\n",
        "image_caption_model = tf.keras.models.load_model(saved_model_path)\n",
        "\n",
        "# Define a function to preprocess the image\n",
        "def preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "# Define a function to generate captions for the new image\n",
        "def generate_caption(img_array):\n",
        "    predictions = image_caption_model.predict(img_array)\n",
        "    # Decode the predicted output to generate captions\n",
        "    # Replace this with your own decoding logic\n",
        "    captions = decode_predictions(predictions)\n",
        "    return captions\n",
        "\n",
        "# Example usage\n",
        "img_path = \"/content/WhatsApp Image 2024-03-10 at 02.06.12.jpeg\"\n",
        "img_array = preprocess_image(img_path)\n",
        "captions = generate_caption(img_array)\n",
        "print(\"Generated captions:\", captions)\n"
      ],
      "metadata": {
        "id": "J0ZrBgUSAbLs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}