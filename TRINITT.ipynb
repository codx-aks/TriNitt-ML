{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codx-aks/TriNitt-ML/blob/main/TRINITT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5sX-TxMdgaq"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from textwrap import wrap\n",
        "import pandas as pd\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import (\n",
        "    GRU,\n",
        "    Add,\n",
        "    AdditiveAttention,\n",
        "    Attention,\n",
        "    Concatenate,\n",
        "    Dense,\n",
        "    Embedding,\n",
        "    LayerNormalization,\n",
        "    Reshape,\n",
        "    StringLookup,\n",
        "    TextVectorization,\n",
        ")\n",
        "\n",
        "print(tf.version.VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNZ1kFFtxW_h"
      },
      "outputs": [],
      "source": [
        "# Change these to control the accuracy/speed\n",
        "VOCAB_SIZE = 10000  # use fewer words to speed up convergence\n",
        "ATTENTION_DIM = 512  # size of dense layer in Attention\n",
        "WORD_EMBEDDING_DIM = 128\n",
        "\n",
        "# InceptionResNetV2 takes (224, 224, 3) image as inputs\n",
        "# and return features in (5, 5, 1536) shape\n",
        "FEATURE_EXTRACTOR = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n",
        "    include_top=False, weights=\"imagenet\"\n",
        ")\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "IMG_CHANNELS = 3\n",
        "FEATURES_SHAPE = (5, 5, 1536)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCvcvhc8EzN",
        "outputId": "b71d2b01-f494-42a3-c720-9bcef79e65d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee-dkPrDKpmV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import ast\n",
        "import io\n",
        "\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "def decode_and_resize_image(image_bytes):\n",
        "    img = tf.image.decode_jpeg(image_bytes, channels=3)\n",
        "    img = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
        "    # Normalize pixel values\n",
        "    img = img / 255.0\n",
        "    return img\n",
        "\n",
        "def get_image_label(filename, caption, img):\n",
        "    caption_list = [caption] if not isinstance(caption, list) else caption\n",
        "    first_caption = caption_list[0]\n",
        "    print(len(caption_list))\n",
        "    return {\"image_tensor\": img, \"caption\": first_caption}\n",
        "\n",
        "\n",
        "# Define mapping function\n",
        "def map_fn(filename, caption, image_bytes):\n",
        "    # Decode and resize image\n",
        "    print(\"Type of image_bytes:\", type(image_bytes))\n",
        "    print(\"insideeee\")\n",
        "    img = decode_and_resize_image(image_bytes)\n",
        "    return get_image_label(filename, caption, img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataset\n",
        "dataset_path = \"/content/drive/My Drive/archive/train.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Parse the stringified dictionaries in the \"image\" column\n",
        "df['image'] = df['image'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to extract the image bytes from the nested dictionary\n",
        "def extract_image_bytes(image):\n",
        "    return image.get(\"bytes\", None)\n",
        "\n",
        "# Extract image bytes from the \"image\" column\n",
        "df[\"image_bytes\"] = df[\"image\"].apply(extract_image_bytes)\n",
        "\n",
        "print(\"Data type of image_bytes:\", df[\"image_bytes\"].dtype)\n",
        "print(\"Shape of image_bytes:\", df[\"image_bytes\"].shape)\n",
        "print(\"Head of image_bytes:\")\n",
        "print(df[\"image_bytes\"].head())\n",
        "\n",
        "# Create dataset\n",
        "trainds = tf.data.Dataset.from_tensor_slices((df[\"filename\"], df[\"captions\"], df[\"image_bytes\"]))\n",
        "# Apply mapping function to the dataset\n",
        "trainds = trainds.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE).shuffle(BUFFER_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "bHb4_PmugTdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read dataset\n",
        "dataset_path_test = \"/content/drive/My Drive/archive/test.csv\"\n",
        "dftest = pd.read_csv(dataset_path_test)\n",
        "\n",
        "# Parse the stringified dictionaries in the \"image\" column\n",
        "dftest['image'] = dftest['image'].apply(ast.literal_eval)\n",
        "\n",
        "# Extract image bytes from the \"image\" column\n",
        "dftest[\"image_bytes\"] = dftest[\"image\"].apply(extract_image_bytes)\n",
        "\n",
        "print(\"Data type of image_bytes:\", dftest[\"image_bytes\"].dtype)\n",
        "print(\"Shape of image_bytes:\", dftest[\"image_bytes\"].shape)\n",
        "print(\"Head of image_bytes:\")\n",
        "print(dftest[\"image_bytes\"].head())\n",
        "\n",
        "# Create dataset\n",
        "testds = tf.data.Dataset.from_tensor_slices((dftest[\"filename\"], dftest[\"captions\"], dftest[\"image_bytes\"]))\n",
        "\n",
        "# Apply mapping function to the dataset\n",
        "testds = testds.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE).shuffle(BUFFER_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "l0jP3f7jllP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read dataset\n",
        "dataset_path_val = \"/content/drive/My Drive/archive/valid.csv\"\n",
        "dfval = pd.read_csv(dataset_path_val)\n",
        "\n",
        "# Parse the stringified dictionaries in the \"image\" column\n",
        "dfval['image'] = dfval['image'].apply(ast.literal_eval)\n",
        "\n",
        "# Extract image bytes from the \"image\" column\n",
        "dfval[\"image_bytes\"] = dfval[\"image\"].apply(extract_image_bytes)\n",
        "\n",
        "print(\"Data type of image_bytes:\", dfval[\"image_bytes\"].dtype)\n",
        "print(\"Shape of image_bytes:\", dfval[\"image_bytes\"].shape)\n",
        "print(\"Head of image_bytes:\")\n",
        "print(dfval[\"image_bytes\"].head())\n",
        "\n",
        "# Create dataset\n",
        "valds = tf.data.Dataset.from_tensor_slices((dfval[\"filename\"], dfval[\"captions\"], dfval[\"image_bytes\"]))\n",
        "\n",
        "# Apply mapping function to the dataset\n",
        "valds = valds.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE).shuffle(BUFFER_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "IzJhfDymmBUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jHC1hQpKvbS"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fetch data example\n",
        "for data in valds.take(4):\n",
        "    byte_array = data['image_tensor']\n",
        "    # Convert to NumPy array using TensorFlow's NumPy compatibility feature\n",
        "    byte_array_np = tf.experimental.numpy.asarray(byte_array)\n",
        "    # Convert back to regular NumPy array for image processing\n",
        "    byte_array_np = np.array(byte_array_np)\n",
        "    image = (byte_array_np * 255).astype(np.uint8)\n",
        "\n",
        "    # Extract the caption\n",
        "    caption = data['caption'].numpy().decode('utf-8')  # Decode bytes to string\n",
        "\n",
        "    # Display the image and caption\n",
        "    plt.imshow(image)\n",
        "    plt.title(caption)\n",
        "    plt.axis('off')  # Turn off axis\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xY0128FKzeT"
      },
      "outputs": [],
      "source": [
        "def add_start_end_token(data):\n",
        "    start = tf.convert_to_tensor(\"<start>\")\n",
        "    end = tf.convert_to_tensor(\"<end>\")\n",
        "    data[\"caption\"] = tf.strings.join(\n",
        "        [start, data[\"caption\"], end], separator=\" \"\n",
        "    )\n",
        "    return data\n",
        "\n",
        "\n",
        "trainds = trainds.map(add_start_end_token)\n",
        "testds = testds.map(add_start_end_token)\n",
        "valds = valds.map(add_start_end_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOrt0x_kLJoV"
      },
      "outputs": [],
      "source": [
        "MAX_CAPTION_LEN = 128\n",
        "\n",
        "\n",
        "# We will override the default standardization of TextVectorization to preserve\n",
        "# \"<>\" characters, so we preserve the tokens for the <start> and <end>.\n",
        "def standardize(inputs):\n",
        "    inputs = tf.strings.lower(inputs)\n",
        "    return tf.strings.regex_replace(\n",
        "        inputs, r\"[!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~]?\", \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Choose the most frequent words from the vocabulary & remove punctuation etc.\n",
        "tokenizer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    standardize=standardize,\n",
        "    output_sequence_length=MAX_CAPTION_LEN,\n",
        ")\n",
        "\n",
        "tokenizer.adapt(trainds.map(lambda x: x[\"caption\"]))\n",
        "tokenizer.adapt(valds.map(lambda x: x[\"caption\"]))\n",
        "tokenizer.adapt(testds.map(lambda x: x[\"caption\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4w_6JbALMd3"
      },
      "outputs": [],
      "source": [
        "tokenizer([\"<start> This is a sentence <end>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vygKqIPBLO2-"
      },
      "outputs": [],
      "source": [
        "sample_captions = []\n",
        "for d in trainds.take(5):\n",
        "    sample_captions.append(d[\"caption\"].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZt4vi_7LXKt"
      },
      "outputs": [],
      "source": [
        "for wordid in tokenizer([sample_captions[0]])[0]:\n",
        "    print(tokenizer.get_vocabulary()[wordid], end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHSHjoouLX6B"
      },
      "outputs": [],
      "source": [
        "# Lookup table: Word -> Index\n",
        "word_to_index = StringLookup(\n",
        "    mask_token=\"\", vocabulary=tokenizer.get_vocabulary()\n",
        ")\n",
        "\n",
        "# Lookup table: Index -> Word\n",
        "index_to_word = StringLookup(\n",
        "    mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY5DtJQ9LbEw"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "\n",
        "def create_ds_fn(data):\n",
        "    img_tensor = data[\"image_tensor\"]\n",
        "    caption = tokenizer(data[\"caption\"])\n",
        "\n",
        "    target = tf.roll(caption, -1, 0)\n",
        "    zeros = tf.zeros([1], dtype=tf.int64)\n",
        "    target = tf.concat((target[:-1], zeros), axis=-1)\n",
        "    return (img_tensor, caption), target\n",
        "\n",
        "\n",
        "batched_ds = (\n",
        "    trainds.map(create_ds_fn)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "batched_val_ds = (\n",
        "    valds.map(create_ds_fn)\n",
        "    .batch(128, drop_remainder=True)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "batched_test_ds = (\n",
        "    testds.map(create_ds_fn)\n",
        "    .batch(128, drop_remainder=True)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7olM63LLiB9"
      },
      "outputs": [],
      "source": [
        "print(len(batched_val_ds))\n",
        "for (img, caption), label in batched_val_ds.take(2):\n",
        "    print(f\"Image shape: {img.shape}\")\n",
        "    print(f\"Caption shape: {caption.shape}\")\n",
        "    print(f\"Label shape: {label.shape}\")\n",
        "    print(caption[0])\n",
        "    print(label[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxdpEyEQLlD3"
      },
      "outputs": [],
      "source": [
        "FEATURE_EXTRACTOR.trainable = False\n",
        "\n",
        "image_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "image_features = FEATURE_EXTRACTOR(image_input)\n",
        "\n",
        "x = Reshape((FEATURES_SHAPE[0] * FEATURES_SHAPE[1], FEATURES_SHAPE[2]))(image_features)\n",
        "encoder_output = Dense(ATTENTION_DIM, activation=\"relu\")(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZeLl8ebLoUa"
      },
      "outputs": [],
      "source": [
        "encoder = tf.keras.Model(inputs=image_input, outputs=encoder_output)\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t8PIRR5LviD"
      },
      "outputs": [],
      "source": [
        "word_input = Input(shape=(MAX_CAPTION_LEN), name=\"words\")\n",
        "embed_x = Embedding(VOCAB_SIZE, ATTENTION_DIM)(word_input)\n",
        "\n",
        "decoder_gru = GRU(\n",
        "    ATTENTION_DIM,\n",
        "    return_sequences=True,\n",
        "    return_state=True,\n",
        ")\n",
        "gru_output, gru_state = decoder_gru(embed_x)\n",
        "\n",
        "decoder_attention = Attention()\n",
        "context_vector = decoder_attention([gru_output, encoder_output])\n",
        "\n",
        "addition = Add()([gru_output, context_vector])\n",
        "\n",
        "layer_norm = LayerNormalization(axis=-1)\n",
        "layer_norm_out = layer_norm(addition)\n",
        "\n",
        "decoder_output_dense = Dense(VOCAB_SIZE)\n",
        "decoder_output = decoder_output_dense(layer_norm_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSvh1iJXLwYo"
      },
      "outputs": [],
      "source": [
        "decoder = tf.keras.Model(\n",
        "    inputs=[word_input, encoder_output], outputs=decoder_output\n",
        ")\n",
        "# tf.keras.utils.plot_model(decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVUPN2NmL1la"
      },
      "outputs": [],
      "source": [
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6ohhtXIL4lG"
      },
      "outputs": [],
      "source": [
        "image_caption_train_model = tf.keras.Model(\n",
        "    inputs=[image_input, word_input], outputs=decoder_output\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0TeeMwyL7Uz"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    # returns 1 to word index and 0 to padding (e.g. [1,1,1,1,1,0,0,0,0,...,0])\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int32)\n",
        "    sentence_len = tf.reduce_sum(mask)\n",
        "    loss_ = loss_[:sentence_len]\n",
        "\n",
        "    return tf.reduce_mean(loss_, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsChe_k_L-Jc"
      },
      "outputs": [],
      "source": [
        "image_caption_train_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=loss_function,\n",
        "     metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "kviA8O74YjaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate sacrebleu"
      ],
      "metadata": {
        "id": "vtxB7dKQQ5PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMMh003HMBFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d120b56a-8702-47cc-f4a5-77a081221dfa"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29/34 [========================>.....] - ETA: 8:16 - loss: 3.3487 - accuracy: 0.5428"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "# history = image_caption_train_model.fit(batched_ds, epochs=1)\n",
        "EPOCHS=1\n",
        "# Train the model\n",
        "history = image_caption_train_model.fit(batched_ds, epochs=EPOCHS, validation_data=batched_val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcqmfJJiBzAk"
      },
      "outputs": [],
      "source": [
        "image_caption_train_model.save('/content/drive/My Drive/gru_model2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = image_caption_train_model.evaluate(batched_test_ds)\n",
        "\n",
        "print(\"Test loss:\", test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQcZzVcfv6a0",
        "outputId": "9d83b872-0b49-4093-f08a-e7acd10b8c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/8 [==============>...............] - ETA: 2:36 - loss: 2.9331 - accuracy: 0.5218"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# Plot the training and validation accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accuracy, label='Training Accuracy', color='blue')\n",
        "plt.plot(val_accuracy, label='Validation Accuracy', color='red')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss, label='Training Loss', color='blue')\n",
        "plt.plot(val_loss, label='Validation Loss', color='red')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-GCvl4euBdrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "\n",
        "def create_ds_fn(data):\n",
        "    img_tensor = data[\"image_tensor\"]\n",
        "    caption = tokenizer(data[\"caption\"])\n",
        "\n",
        "    target = tf.roll(caption, -1, 0)\n",
        "    zeros = tf.zeros([1], dtype=tf.int64)\n",
        "    target = tf.concat((target[:-1], zeros), axis=-1)\n",
        "    return (img_tensor, caption), target\n",
        "\n",
        "\n",
        "batched_val_ds = (\n",
        "    valds.map(create_ds_fn)\n",
        "    .batch(128, drop_remainder=True)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "batched_test_ds = (\n",
        "    testds.map(create_ds_fn)\n",
        "    .batch(128, drop_remainder=True)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "xjmb3K_tiM2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    # returns 1 to word index and 0 to padding (e.g. [1,1,1,1,1,0,0,0,0,...,0])\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int32)\n",
        "    sentence_len = tf.reduce_sum(mask)\n",
        "    loss_ = loss_[:sentence_len]\n",
        "\n",
        "    return tf.reduce_mean(loss_, 1)\n",
        "\n",
        "saved_model_path = '/content/drive/My Drive/gru_model.h5'\n",
        "loaded_model = tf.keras.models.load_model(saved_model_path,custom_objects={'loss_function': loss_function})\n",
        "\n",
        "# Assuming batched_val_ds is your validation dataset\n",
        "# Make predictions on the validation dataset\n",
        "predictions_val = loaded_model.predict(valds)\n",
        "\n",
        "# Convert predictions to a DataFrame\n",
        "predictions_dfval = pd.DataFrame(predictions_val)\n",
        "\n",
        "# Save DataFrame to a CSV file\n",
        "csv_filename_val = \"/content/drive/My Drive/validation_predictions.csv\"\n",
        "predictions_dfval.to_csv(csv_filename_val, index=False)\n",
        "\n",
        "print(\"Shape of validation predictions:\", predictions_val.shape)\n",
        "print(\"Predictions saved to:\", csv_filename_val)"
      ],
      "metadata": {
        "id": "FthuSgl_wh7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_test = image_caption_train_model.predict(batched_test_ds)\n",
        "\n",
        "predictions_dftest = pd.DataFrame(predictions_test)\n",
        "\n",
        "# Save DataFrame to a CSV file\n",
        "csv_filename_test = \"/content/drive/My Drive/test_predictions.csv\"\n",
        "predictions_dftest.to_csv(csv_filename_test, index=False)\n",
        "print(\"Shape of test predictions:\", predictions_test.shape)"
      ],
      "metadata": {
        "id": "SvLMTrQMwkxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def tokenizer_config_to_json(tokenizer_config):\n",
        "    serialized_config = {}\n",
        "    for key, value in tokenizer_config.items():\n",
        "        if not callable(value):  # Exclude functions from serialization\n",
        "            if isinstance(value, dict):\n",
        "                serialized_value = tokenizer_config_to_json(value)  # Recursive call for nested dictionaries\n",
        "            else:\n",
        "                serialized_value = value\n",
        "            serialized_config[key] = serialized_value\n",
        "    return serialized_config\n",
        "\n",
        "file_path = \"/content/drive/My Drive/tokenizer.json\"\n",
        "tokenizer_config_json = tokenizer_config_to_json(tokenizer.get_config())\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(tokenizer_config_json, json_file)"
      ],
      "metadata": {
        "id": "EqIMmMwwCxev"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "with open(file_path, 'r') as json_file:\n",
        "    loaded_tokenizer_config = json.load(json_file)\n",
        "print(loaded_tokenizer_config)\n",
        "def custom_standardization(inputs):\n",
        "    inputs = tf.strings.lower(inputs)\n",
        "    return tf.strings.regex_replace(\n",
        "        inputs, r\"[!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~]?\", \"\"\n",
        "    )\n",
        "# Reconstruct tokenizer from configuration\n",
        "tokenizer = TextVectorization.from_config(loaded_tokenizer_config)\n",
        "tokenizer.standardize = custom_standardization"
      ],
      "metadata": {
        "id": "AZBXTHOwa0iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecqckvjnMDug"
      },
      "outputs": [],
      "source": [
        "gru_state_input = Input(shape=(ATTENTION_DIM), name=\"gru_state_input\")\n",
        "\n",
        "# Reuse trained GRU, but update it so that it can receive states.\n",
        "gru_output, gru_state = decoder_gru(embed_x, initial_state=gru_state_input)\n",
        "\n",
        "# Reuse other layers as well\n",
        "context_vector = decoder_attention([gru_output, encoder_output])\n",
        "addition_output = Add()([gru_output, context_vector])\n",
        "layer_norm_output = layer_norm(addition_output)\n",
        "\n",
        "decoder_output = decoder_output_dense(layer_norm_output)\n",
        "\n",
        "# Define prediction Model with state input and output\n",
        "decoder_pred_model = tf.keras.Model(\n",
        "    inputs=[word_input, gru_state_input, encoder_output],\n",
        "    outputs=[decoder_output, gru_state],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image\n",
        "import json\n",
        "def decode_predictions(predictions):\n",
        "    generated_captions = []\n",
        "    with open('tokenizer.json', 'r') as json_file:\n",
        "        tokenizer_json = json.load(json_file)\n",
        "    tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)\n",
        "    for prediction in predictions:\n",
        "        caption_words = []\n",
        "        for token_id in prediction:\n",
        "            word = tokenizer.index_word.get(token_id, '<UNK>')\n",
        "            caption_words.append(word)\n",
        "            if word == '<end>':\n",
        "                break\n",
        "        caption = ' '.join(caption_words[:-1])\n",
        "        generated_captions.append(caption)\n",
        "\n",
        "    return generated_captions\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERMH7-24uoem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu>=1.4.12"
      ],
      "metadata": {
        "id": "dcvHPyJEPzrK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4YRYG-LShiN",
        "outputId": "d72750d4-6621-4de6-d611-d7f6f31c50b7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def compute_bleu_scores(predictions, references):\n",
        "    references = [[ref.split()] for ref in references]\n",
        "    predictions = [pred.split() for pred in predictions]\n",
        "    bleu_score = corpus_bleu(references, predictions)\n",
        "    return bleu_score\n",
        "\n",
        "def evaluate_model(model, dataset):\n",
        "    total_bleu_score = 0\n",
        "    references_list = []\n",
        "    predictions_list = []\n",
        "\n",
        "    # Batch the dataset before iterating over it\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    for batch in dataset:\n",
        "        image_tensors, captions = batch['image_tensor'], batch['caption']\n",
        "        for image_tensor, caption in zip(image_tensors, captions):\n",
        "            # Predict caption for the current image\n",
        "            predicted_caption = predict_caption(tf.expand_dims(image_tensor, axis=0))\n",
        "\n",
        "            # Add reference and prediction for BLEU score calculation\n",
        "            references_list.append([caption.numpy().decode(\"utf-8\")])\n",
        "            predictions_list.append(predicted_caption)\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu_score = compute_bleu_scores(predictions_list, references_list)\n",
        "\n",
        "    return bleu_score\n"
      ],
      "metadata": {
        "id": "pbbFtYeg1YHn"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "thEbmuWzTcOI",
        "outputId": "065c4ffa-98a4-46ad-eb3a-8e8606921015"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-62f13835-cccc-479b-813c-3e4422b5099b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-62f13835-cccc-479b-813c-3e4422b5099b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving WhatsApp Image 2024-03-10 at 02.06.12.jpeg to WhatsApp Image 2024-03-10 at 02.06.12.jpeg\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIzyx6PhMK-T"
      },
      "outputs": [],
      "source": [
        "MINIMUM_SENTENCE_LENGTH = 5\n",
        "\n",
        "\n",
        "## Probabilistic prediction using the trained model\n",
        "def predict_caption(filename):\n",
        "    gru_state = tf.zeros((1, ATTENTION_DIM))\n",
        "\n",
        "    img = tf.image.decode_jpeg(tf.io.read_file(filename), channels=IMG_CHANNELS)\n",
        "    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
        "    img = img / 255\n",
        "\n",
        "    features = encoder(tf.expand_dims(img, axis=0))\n",
        "    dec_input = tf.expand_dims([word_to_index(\"<start>\")], 1)\n",
        "    result = []\n",
        "    for i in range(MAX_CAPTION_LEN):\n",
        "        predictions, gru_state = decoder_pred_model([dec_input, gru_state, features])\n",
        "\n",
        "        # Get the index with maximum probability\n",
        "        predicted_id = tf.argmax(predictions[0][0]).numpy()\n",
        "\n",
        "        result.append(tokenizer.get_vocabulary()[predicted_id])\n",
        "\n",
        "        if predicted_id == word_to_index(\"<end>\"):\n",
        "            break\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 1)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_caption_train_model.summary()\n"
      ],
      "metadata": {
        "id": "Zif0CC175aOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRhmnCTmTddj"
      },
      "outputs": [],
      "source": [
        "filename = \"/content/WhatsApp Image 2024-03-10 at 02.06.12.jpeg\"\n",
        "for i in range(5):\n",
        "    image, caption = predict_caption(filename)\n",
        "    print(\" \".join(caption[:-1]) + \".\")\n",
        "\n",
        "img = tf.image.decode_jpeg(tf.io.read_file(filename), channels=IMG_CHANNELS)\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "saved_model_path = \"/content/drive/My Drive/gru_model.h5\"\n",
        "image_caption_model = tf.keras.models.load_model(saved_model_path)\n",
        "\n",
        "# Define a function to preprocess the image\n",
        "def preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "# Define a function to generate captions for the new image\n",
        "def generate_caption(img_array):\n",
        "    predictions = image_caption_model.predict(img_array)\n",
        "    # Decode the predicted output to generate captions\n",
        "    # Replace this with your own decoding logic\n",
        "    captions = decode_predictions(predictions)\n",
        "    return captions\n",
        "\n",
        "# Example usage\n",
        "img_path = \"/content/WhatsApp Image 2024-03-10 at 02.06.12.jpeg\"\n",
        "img_array = preprocess_image(img_path)\n",
        "captions = generate_caption(img_array)\n",
        "print(\"Generated captions:\", captions)\n"
      ],
      "metadata": {
        "id": "J0ZrBgUSAbLs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}